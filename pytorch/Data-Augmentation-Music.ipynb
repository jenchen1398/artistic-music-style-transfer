{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Here we use the librosa library to implement the data augmentation recommended by the Facebook FAIR A Universal Music Translation Network paper. We import music, split it into (by default) 1 second clips, and then randomly shift [0.25, .5] second clips up or down by [-0.5, .5] half-steps. \n",
    "\n",
    "This is implemented as a PyTorch Dataset, which can be fed (as shown) into a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa as libr\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Music\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, sr = 22050, clip_length = 1, range = 0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the music.\n",
    "            sr (int): Sampling rate (all music will be resampled to this rate by default. Default = 22050)\n",
    "            clip_length (float): Clip length in seconds\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.sr = sr\n",
    "        self.clip_length = clip_length\n",
    "        self.range = range\n",
    "        \n",
    "        allowed_formats = ['.m4a', '.wav', '.mp3']\n",
    "        \n",
    "        data = []\n",
    "        \n",
    "        for file in os.listdir(self.root_dir):\n",
    "            print(file)\n",
    "            if not any((file.endswith(ext) for ext in allowed_formats)):\n",
    "                continue\n",
    "    \n",
    "            try:\n",
    "                X, sr = libr.load(\"{}\\{}\".format(root_dir, file), self.sr)\n",
    "                assert(sr == self.sr)\n",
    "                Y = libr.util.frame(X, self.sr * self.clip_length) # split into 1 second clips\n",
    "                data.append(Y)\n",
    "                print(\"successfully loaded {} {}-second ({} sample) clip(s) from {}\".format(Y.shape[1], self.clip_length, self.clip_length * self.sr, file))\n",
    "            except AssertionError as e:\n",
    "                print(\"unable to load {}\".format(file))\n",
    "                \n",
    "        self.data = np.concatenate(data, axis = 1).T\n",
    "        \n",
    "        # to speed this up, maybe something like this, i.e. augment first\n",
    "        \n",
    "#         pitch = np.random.random_sample(self.data.shape[1]) - 0.5 # how much to raise/lower by\n",
    "#         dur = np.random.random_sample(self.data.shape[1]) / 4 + 0.25 # duration of subsample between [0.25, .5]\n",
    "#         low = min(np.random.random_sample(self.data.shape[1]), 1 - dur) # lower bound\n",
    "        \n",
    "#         a = np.round(self.sr * low, 0)\n",
    "#         b = np.round(self.sr * dur, 0) + a\n",
    "        \n",
    "#         clip[:, a : b] = libr.effects.pitch_shift(clip[:, a : b], self.sr, n_steps = pitch) # may modify data matrix, not a huge deal\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pitch = self.range * 2 * (np.random.random_sample() - 0.5) # how much to raise/lower by\n",
    "        dur = (np.random.random_sample() / 4 + 0.25) * self.clip_length # duration of subsample between [0.25, .5]\n",
    "        low = min(self.clip_length * np.random.random_sample(), self.clip_length - dur) # lower bound\n",
    "        \n",
    "        clip = self.data[idx]\n",
    "        \n",
    "        a = int(self.sr * low)\n",
    "        b = int(self.sr * dur) + a\n",
    "        \n",
    "        clip[a : b] = libr.effects.pitch_shift(clip[a : b], self.sr, n_steps = pitch) # may modify data matrix, not a huge deal\n",
    "        print('hi')\n",
    "        return torch.Tensor(clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To create a dataset:\n",
    "\n",
    "Pass it the path to the audio files and optionally some command line arguments. \n",
    "\n",
    "Generating the dataset takes a while. This takes so long because it uses librosa which resamples automatically. If you are using a bunch of audio with the same sampling rate, you can use scipy instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09 Bach_ Violin Partita #2 In D Mino.m4a\n",
      "successfully loaded 5517 1-second (22050 sample) clip(s) from 09 Bach_ Violin Partita #2 In D Mino.m4a\n",
      "12 Bach_ Violin Partita #2 In D Mino.m4a\n",
      "successfully loaded 46125 1-second (22050 sample) clip(s) from 12 Bach_ Violin Partita #2 In D Mino.m4a\n",
      "13 Bach_ Sonata #3 In C For Solo Vio.m4a\n",
      "successfully loaded 12666 1-second (22050 sample) clip(s) from 13 Bach_ Sonata #3 In C For Solo Vio.m4a\n",
      "15 Bach_ Sonata #3 In C For Solo Vio.m4a\n",
      "successfully loaded 10157 1-second (22050 sample) clip(s) from 15 Bach_ Sonata #3 In C For Solo Vio.m4a\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-0e1b1af789d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMusicDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'music_data/Hahn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0md2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMusicDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'music_data/Milstein'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0md2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-4f23e91b56bb>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root_dir, sr, clip_length, range)\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unable to load {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# to speed this up, maybe something like this, i.e. augment first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d1 = MusicDataset('music_data/Hahn', clip_length = 1)\n",
    "d2 = MusicDataset('music_data/Milstein', clip_length = 1)\n",
    "d2[1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-76476917e82c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hahn_dataset.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'milstein_dataset.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('hahn_dataset.pkl', 'wb') as f1:\n",
    "    pkl.dump(d1, f, pkl.HIGHEST_PROTOCOL)\n",
    "with open('milstein_dataset.pkl', 'wb') as f2:\n",
    "    pkl.dump(d2, f, pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('perlman_dataset.pkl', 'wb') as f:\n",
    "    pkl.dump(d, f, pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10142 19298 -0.446739963880348\n",
      "4376 15172 -0.4925592750895442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "libr.output.write_wav('example2.wav', d[1900].numpy(), 22050)\n",
    "type(d[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "PyTorch DataLoaders allow you do load a bunch of data and iterate over it. For example, you can do\n",
    "\n",
    "```\n",
    "for minibatch in dataloader:\n",
    "    train\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(d, batch_size=10, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(self, idx):\n",
    "    pitch = self.range * 2 * (np.random.random_sample() - 0.5) # how much to raise/lower by\n",
    "    dur = (np.random.random_sample() / 4 + 0.25) * self.clip_length # duration of subsample between [0.25, .5]\n",
    "    low = min(self.clip_length * np.random.random_sample(), self.clip_length - dur) # lower bound\n",
    "\n",
    "    clip = self.data[idx]\n",
    "\n",
    "    a = int(self.sr * low)\n",
    "    b = int(self.sr * dur) + a\n",
    "    \n",
    "    print(a, b, pitch)\n",
    "    \n",
    "    clip[a : b] = libr.effects.pitch_shift(clip[a : b], self.sr, n_steps = pitch) # may modify data matrix, not a huge deal\n",
    "\n",
    "    return torch.Tensor(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MusicDataset.__getitem__ = foo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-c728acd251a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write an example to a wav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "libr.output.write_wav('example.wav', sample[3].numpy(), 22050)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
